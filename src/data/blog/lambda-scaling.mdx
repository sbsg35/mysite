---
featured: true
title: 'AWS Lambda, at scale'
summary: 'Do not rely on AWS Lambda to infinitely scale.'
published: '05/05/2024'
last_modified: '05/05/2024'
author_name: 'Raj Ghuman'
author_image: '/static/avatar.jpeg'
thumbnail: '/static/aws-lambda.png'
tags: ['AWS', 'Lambda', 'Scalability', 'Devops']
keywords: ['AWS Lambda', 'Scaling']
---

I had thought AWS Lambda service could handle any amount of scale. I was wrong.
Before you rely on AWS Lambda to infinitely scale, understand these concepts:

- Concurrency
- Provisioned concurrency
- Reserved concurrency
- Burst concurrency

## Concurrency
> Concurrency is the number of in-flight requests that your Lambda function is handling at the same time. Every account has a total concurrency limit of 1,000 concurrent executions across all functions in an AWS Region (contact AWS support to modify this).

Lambda creates more execution environments according to the number of concurrent requests.

If a Lambda function takes 1 second to process request, and there are 100 requests per second, the concurrency is 100.
Throttling occurs after the default concurrency limit of 1000 is reached.

## Provisioned concurrency
Sets the minimum number of execution environments (EE). Prewarms your functions and good for anticipated traffic spikes.
It can also help prevent cold starts. The cost is higher since there are EE's always running.

## Reserved concurrency
Use this flag to set a max concurrency limit per function. 
Using this can prevent a single function from using all account level concurrency.


## Burst concurrency
Lambda your concurrency **scaling rate** is 1,000 EE instances every 10 seconds.
For extremely high volume spiky traffic, this can cause issues. 
In my current job, a microservice using Lambda had very spiky high volume traffic. It would often run into throttling errors, because it could not scale fast enough.
I rebuilt this microservice, but that will be a seperate post.